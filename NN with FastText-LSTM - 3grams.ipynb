{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.0.0 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import fasttext\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.callbacks import History\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score, mean_squared_error, log_loss\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('../classifier.hdf5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "stopping = EarlyStopping(monitor='val_loss', verbose=1, patience=10)\n",
    "history = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('all_train_data.csv')\n",
    "df['label'] = df['label'] - 1\n",
    "df = df.replace(r'\\n',' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = fasttext.load_model('ft_native_300_ru_twitter_nltk_word_tokenize.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 204 ms, sys: 47.7 ms, total: 251 ms\n",
      "Wall time: 316 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "def get_3grams(text):\n",
    "    return [''.join(ngram) for ngram in ngrams(text, 3)]\n",
    "\n",
    "if False:\n",
    "    token2index_3gram = {}\n",
    "    for text in df['text']:\n",
    "        for ngram in get_3grams(text):\n",
    "            if ngram not in token2index_3gram:\n",
    "                token2index_3gram[ngram] = len(token2index_3gram)\n",
    "    \n",
    "    embedding_matrix_3gram = np.zeros((len(token2index_3gram) + 1, ft.get_dimension()))\n",
    "    for token, index in token2index_3gram.items():\n",
    "        embedding_matrix_3gram[index] = ft.get_word_vector(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_3gram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# tokens = ft.get_labels()\n",
    "# embedding_matrix = np.zeros((len(tokens) + 1, ft.get_dimension()))\n",
    "# token2index = {}\n",
    "# for i, token in enumerate(tokens):\n",
    "#     print(i, end='\\r')\n",
    "#     embedding_matrix[i] = ft.get_word_vector(token)\n",
    "#     token2index[token] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "#with open('token2index_3gram.json', 'w') as fp:\n",
    "    #json.dump(token2index_3gram, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('token2index.json', 'r') as fp:\n",
    "    token2index = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('embedding_matrix_3gram.npy', embedding_matrix_3gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_3gram = np.load('embedding_matrix_3gram.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_3gram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = []\n",
    "\n",
    "for row in text:\n",
    "    indexes = []\n",
    "    for elem in row:\n",
    "        token_index = token2index.get(elem, embedding_matrix.shape[0] - 1)\n",
    "        indexes.append(token_index)\n",
    "    X.append(indexes)\n",
    "\n",
    "X = pad_sequences(list(X))\n",
    "       \n",
    "y = pd.get_dummies(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalMaxPooling1D, MaxPooling1D, LSTM, SpatialDropout1D\n",
    "\n",
    "class BaseIntentModel:\n",
    "    def __init__(self, seed=42): \n",
    "        self.seed = seed\n",
    "        self.model = self.build_model()\n",
    "        self.seed_everything(seed)\n",
    "        \n",
    "    def fit(self, X_train, Y_train, X_val, Y_val):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def fit_all_data(self, X_train, Y_train):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def build_model(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def seed_everything(self, seed):\n",
    "        random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "        \n",
    "class NeuralNetFastText(BaseIntentModel):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embedding_matrix_3gram = np.load('embedding_matrix_3gram.npy')\n",
    "        super(NeuralNetFastText, self).__init__()\n",
    "        self.token2index = json.load(open('token2index_3gram.json', 'rb'))\n",
    "    \n",
    "    def prepare_x(self, X_raw):\n",
    "        \n",
    "        data = X_raw['text']\n",
    "        text = [re.sub(r'([^\\s\\w]|_)+', '', sentence) for sentence in data]\n",
    "        text = [get_3grams(sentence) for sentence in text]\n",
    "\n",
    "        X = []\n",
    "        for row in text:\n",
    "            indexes = []\n",
    "            for elem in row:\n",
    "                token_index = self.token2index.get(elem, self.embedding_matrix_3gram.shape[0] - 1)\n",
    "                indexes.append(token_index)\n",
    "            X.append(indexes)\n",
    "        X = pad_sequences(list(X), maxlen=200)\n",
    "        return X\n",
    "    \n",
    "    def prepare_y(self, Y_raw):\n",
    "        Y = keras.utils.to_categorical(Y_raw, num_classes=14, dtype='float32')\n",
    "        return Y\n",
    "        \n",
    "    \n",
    "    def build_model(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Embedding(embedding_matrix_3gram.shape[0], 100,input_length=200,weights=[embedding_matrix_3gram],trainable=False))\n",
    "        model.add(SpatialDropout1D(0.2))\n",
    "        model.add(LSTM(300, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "        model.add(LSTM(100, dropout=0.1, recurrent_dropout=0.1))\n",
    "        \n",
    "       \n",
    "        model.add(Dense(14))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, Y_train, X_val, Y_val):\n",
    "        X_train = self.prepare_x(X_train)\n",
    "        X_val = self.prepare_x(X_val)\n",
    "        Y_train = self.prepare_y(Y_train)\n",
    "        Y_val = self.prepare_y(Y_val)\n",
    "        \n",
    "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        callbacks_list = [checkpoint, stopping, history]\n",
    "        print(X_train.shape, Y_train.shape)\n",
    "        print(X_val.shape, Y_val.shape)\n",
    "        self.model.fit(X_train, Y_train, batch_size=5, epochs=20, callbacks=callbacks_list, validation_data=(X_val, Y_val), verbose=0)\n",
    "        \n",
    "        \n",
    "    def fit_all_data(self, X_train, Y_train):\n",
    "        self.model.fit_all_data(X_train, Y_train)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self.prepare_x(X)\n",
    "        Y_pred = self.model.predict(X)\n",
    "        return Y_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = NeuralNetFastText()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 100)          1635000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200, 300)          481200    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                1414      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14)                0         \n",
      "=================================================================\n",
      "Total params: 2,278,014\n",
      "Trainable params: 643,014\n",
      "Non-trainable params: 1,635,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "c.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify(df):\n",
    "    \"\"\"\n",
    "    Принимает на вход датафрейм. Отдает разбитый на стратифай фолды датафрейм(список).\n",
    "    \"\"\"\n",
    "    result=[]\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=123)\n",
    "    for train_index, val_index in skf.split(df, df['label']):\n",
    "        result.append((train_index, val_index))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(Y_true, Y_pred):\n",
    "        Y_true = np.array(Y_true)\n",
    "        Y_pred = np.array(Y_pred)\n",
    "    \n",
    "        print('---------------Classification_report------------------\\n', classification_report(Y_true, [_.argmax() for _ in np.array(Y_pred)], digits=4))\n",
    "        print('RMSE:', mean_squared_error(to_categorical(Y_true, num_classes=14), Y_pred, squared=False))\n",
    "        print('Logloss:', log_loss(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(df, holdout_train_index, holdout_index):\n",
    "    df_holdout = df.loc[holdout_index]\n",
    "    X_holdout = df_holdout[['text']] \n",
    "    Y_holdout = df_holdout['label']\n",
    "    \n",
    "    holdout_predict = np.zeros((df_holdout.shape[0], 14))\n",
    "    oof_true = []\n",
    "    oof_pred = []\n",
    "\n",
    "    for train_index, val_index in stratify(df.loc[holdout_train_index]):\n",
    "        \"\"\"\n",
    "        Извлекаем по очереди K-fold\n",
    "        \"\"\"\n",
    "        df_val = df.loc[val_index]\n",
    "        df_train = df.loc[train_index]           \n",
    "                   \n",
    "        \"\"\"\n",
    "        Извлекаем по очереди K-fold для Holdout\n",
    "        \"\"\"  \n",
    "        X_train = df_train[['text']] \n",
    "        Y_train = df_train['label']\n",
    "        X_val = df_val[['text']]\n",
    "        Y_val = df_val['label']           \n",
    "        \n",
    "        \"\"\"\n",
    "        Фитим модель на X_train и y_val на каждом фолде\n",
    "        \"\"\"\n",
    "        clf = NeuralNetFastText()\n",
    "        clf.fit(X_train, Y_train, X_val, Y_val)\n",
    "        \n",
    "        \"\"\"\n",
    "        Предиктим и скорим модель на каждом фолде\n",
    "        \"\"\"\n",
    "        Y_pred = clf.predict(X_val)\n",
    "    #     scoring(Y_val, Y_pred)\n",
    "    \n",
    "        \"\"\"\n",
    "        OUT OF FOLD\n",
    "        \"\"\"\n",
    "        oof_true.extend(Y_val)\n",
    "        oof_pred.extend(Y_pred)\n",
    "    \n",
    "        holdout_predict += 0.2 * clf.predict(X_holdout)\n",
    "        \n",
    "    return oof_true, oof_pred, Y_holdout, holdout_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12793, 200) (12793, 14)\n",
      "(3199, 200) (3199, 14)\n",
      "(12793, 200) (12793, 14)\n",
      "(3199, 200) (3199, 14)\n",
      "(12794, 200) (12794, 14)\n",
      "(3198, 200) (3198, 14)\n",
      "(12794, 200) (12794, 14)\n",
      "(3198, 200) (3198, 14)\n",
      "(12794, 200) (12794, 14)\n",
      "(3198, 200) (3198, 14)\n",
      "Fold 0:\n",
      "---------------Classification_report------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7200    0.7937    0.7550      4037\n",
      "           1     0.6865    0.6147    0.6486      2201\n",
      "           2     0.6096    0.5469    0.5765      1739\n",
      "           3     0.6401    0.4590    0.5346       891\n",
      "           4     0.7900    0.9001    0.8415      1542\n",
      "           5     0.5778    0.2600    0.3586       100\n",
      "           6     0.6048    0.5498    0.5760       231\n",
      "           7     0.6932    0.7000    0.6966       710\n",
      "           8     0.7680    0.8084    0.7877      2088\n",
      "           9     0.8186    0.7548    0.7854       526\n",
      "          10     0.7270    0.6694    0.6970       366\n",
      "          11     0.5730    0.5721    0.5725       645\n",
      "          12     0.7422    0.8366    0.7866       857\n",
      "          13     0.2308    0.0508    0.0833        59\n",
      "\n",
      "    accuracy                         0.7112     15992\n",
      "   macro avg     0.6558    0.6083    0.6214     15992\n",
      "weighted avg     0.7051    0.7112    0.7052     15992\n",
      "\n",
      "RMSE: 0.17322719\n",
      "Logloss: 0.9255565986103542\n",
      "---------------Classification_report------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8082    0.8922    0.8481      1020\n",
      "           1     0.8437    0.7545    0.7966       558\n",
      "           2     0.8249    0.6903    0.7516       423\n",
      "           3     0.8072    0.6036    0.6907       222\n",
      "           4     0.8492    0.9556    0.8993       383\n",
      "           5     1.0000    0.4444    0.6154        27\n",
      "           6     0.7586    0.7586    0.7586        58\n",
      "           7     0.8177    0.8268    0.8222       179\n",
      "           8     0.8719    0.9078    0.8895       510\n",
      "           9     0.9268    0.8636    0.8941       132\n",
      "          10     0.9091    0.8421    0.8743        95\n",
      "          11     0.7011    0.7722    0.7349       158\n",
      "          12     0.7888    0.9041    0.8426       219\n",
      "          13     1.0000    0.3333    0.5000        15\n",
      "\n",
      "    accuracy                         0.8275      3999\n",
      "   macro avg     0.8505    0.7535    0.7799      3999\n",
      "weighted avg     0.8296    0.8275    0.8242      3999\n",
      "\n",
      "RMSE: 0.1357935478191105\n",
      "Logloss: 0.539054881301558\n",
      "(12794, 200) (12794, 14)\n",
      "(3199, 200) (3199, 14)\n",
      "(12794, 200) (12794, 14)\n",
      "(3199, 200) (3199, 14)\n",
      "(12794, 200) (12794, 14)\n",
      "(3199, 200) (3199, 14)\n",
      "(12795, 200) (12795, 14)\n",
      "(3198, 200) (3198, 14)\n",
      "(12795, 200) (12795, 14)\n",
      "(3198, 200) (3198, 14)\n",
      "Fold 1:\n",
      "---------------Classification_report------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7270    0.7751    0.7503      4037\n",
      "           1     0.6765    0.6431    0.6594      2202\n",
      "           2     0.6244    0.5354    0.5765      1739\n",
      "           3     0.6086    0.5095    0.5547       891\n",
      "           4     0.7837    0.9047    0.8399      1542\n",
      "           5     0.6250    0.3000    0.4054       100\n",
      "           6     0.6453    0.5671    0.6037       231\n",
      "           7     0.6861    0.7141    0.6998       710\n",
      "           8     0.7809    0.8008    0.7907      2088\n",
      "           9     0.8175    0.7833    0.8000       526\n",
      "          10     0.7447    0.6694    0.7050       366\n",
      "          11     0.5802    0.5721    0.5761       645\n",
      "          12     0.7474    0.8390    0.7905       857\n",
      "          13     0.1765    0.0508    0.0789        59\n",
      "\n",
      "    accuracy                         0.7136     15993\n",
      "   macro avg     0.6588    0.6189    0.6308     15993\n",
      "weighted avg     0.7077    0.7136    0.7086     15993\n",
      "\n",
      "RMSE: 0.172687\n",
      "Logloss: 0.9221998384498094\n",
      "---------------Classification_report------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8110    0.8803    0.8442      1019\n",
      "           1     0.8209    0.7312    0.7735       558\n",
      "           2     0.7889    0.6698    0.7245       424\n",
      "           3     0.7857    0.6441    0.7079       222\n",
      "           4     0.8143    0.9843    0.8913       383\n",
      "           5     0.8824    0.5556    0.6818        27\n",
      "           6     0.7143    0.7759    0.7438        58\n",
      "           7     0.8343    0.8202    0.8272       178\n",
      "           8     0.8676    0.8863    0.8768       510\n",
      "           9     0.9280    0.8722    0.8992       133\n",
      "          10     0.9022    0.8737    0.8877        95\n",
      "          11     0.7365    0.6899    0.7124       158\n",
      "          12     0.8082    0.9083    0.8553       218\n",
      "          13     1.0000    0.2667    0.4211        15\n",
      "\n",
      "    accuracy                         0.8197      3998\n",
      "   macro avg     0.8353    0.7542    0.7748      3998\n",
      "weighted avg     0.8200    0.8197    0.8161      3998\n",
      "\n",
      "RMSE: 0.13847146501362453\n",
      "Logloss: 0.55956416293101\n",
      "(12794, 200) (12794, 14)\n",
      "(3199, 200) (3199, 14)\n",
      "(12794, 200) (12794, 14)\n",
      "(3199, 200) (3199, 14)\n",
      "(12794, 200) (12794, 14)\n",
      "(3199, 200) (3199, 14)\n",
      "(12795, 200) (12795, 14)\n",
      "(3198, 200) (3198, 14)\n",
      "(12795, 200) (12795, 14)\n",
      "(3198, 200) (3198, 14)\n",
      "Fold 2:\n",
      "---------------Classification_report------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7318    0.7902    0.7599      4037\n",
      "           1     0.6484    0.6698    0.6589      2202\n",
      "           2     0.6599    0.5256    0.5851      1739\n",
      "           3     0.5837    0.5241    0.5523       891\n",
      "           4     0.7830    0.9008    0.8378      1542\n",
      "           5     0.6122    0.3000    0.4027       100\n",
      "           6     0.5979    0.5022    0.5459       231\n",
      "           7     0.7163    0.6577    0.6858       710\n",
      "           8     0.7844    0.7965    0.7904      2088\n",
      "           9     0.8388    0.7814    0.8091       526\n",
      "          10     0.7921    0.6557    0.7175       366\n",
      "          11     0.5802    0.5829    0.5816       645\n",
      "          12     0.7415    0.8098    0.7741       857\n",
      "          13     0.3750    0.0508    0.0896        59\n",
      "\n",
      "    accuracy                         0.7150     15993\n",
      "   macro avg     0.6747    0.6105    0.6279     15993\n",
      "weighted avg     0.7107    0.7150    0.7100     15993\n",
      "\n",
      "RMSE: 0.17320582\n",
      "Logloss: 0.9358657801497585\n",
      "---------------Classification_report------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8163    0.8852    0.8493      1019\n",
      "           1     0.7989    0.7975    0.7982       558\n",
      "           2     0.8285    0.6722    0.7422       424\n",
      "           3     0.7709    0.6216    0.6883       222\n",
      "           4     0.8235    0.9504    0.8824       383\n",
      "           5     0.8824    0.5556    0.6818        27\n",
      "           6     0.7778    0.7241    0.7500        58\n",
      "           7     0.8191    0.8652    0.8415       178\n",
      "           8     0.8446    0.8843    0.8640       510\n",
      "           9     0.9250    0.8346    0.8775       133\n",
      "          10     0.8471    0.7579    0.8000        95\n",
      "          11     0.7733    0.7342    0.7532       158\n",
      "          12     0.8447    0.8486    0.8467       218\n",
      "          13     1.0000    0.2667    0.4211        15\n",
      "\n",
      "    accuracy                         0.8214      3998\n",
      "   macro avg     0.8394    0.7427    0.7712      3998\n",
      "weighted avg     0.8218    0.8214    0.8182      3998\n",
      "\n",
      "RMSE: 0.13674260454213735\n",
      "Logloss: 0.5427853872988595\n",
      "(12794, 200) (12794, 14)\n",
      "(3199, 200) (3199, 14)\n",
      "(12794, 200) (12794, 14)\n",
      "(3199, 200) (3199, 14)\n",
      "(12794, 200) (12794, 14)\n",
      "(3199, 200) (3199, 14)\n",
      "(12795, 200) (12795, 14)\n",
      "(3198, 200) (3198, 14)\n",
      "(12795, 200) (12795, 14)\n",
      "(3198, 200) (3198, 14)\n",
      "Fold 3:\n",
      "---------------Classification_report------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7222    0.7855    0.7525      4037\n",
      "           1     0.6905    0.6190    0.6528      2202\n",
      "           2     0.6453    0.5313    0.5828      1739\n",
      "           3     0.6286    0.4635    0.5336       891\n",
      "           4     0.7857    0.8988    0.8385      1542\n",
      "           5     0.5738    0.3500    0.4348       100\n",
      "           6     0.6127    0.5411    0.5747       231\n",
      "           7     0.7202    0.6634    0.6906       710\n",
      "           8     0.7568    0.8108    0.7829      2088\n",
      "           9     0.7835    0.7776    0.7805       526\n",
      "          10     0.7273    0.6776    0.7016       366\n",
      "          11     0.5339    0.6357    0.5803       645\n",
      "          12     0.7299    0.8355    0.7791       857\n",
      "          13     0.2857    0.0339    0.0606        59\n",
      "\n",
      "    accuracy                         0.7107     15993\n",
      "   macro avg     0.6569    0.6160    0.6247     15993\n",
      "weighted avg     0.7056    0.7107    0.7048     15993\n",
      "\n",
      "RMSE: 0.17340827\n",
      "Logloss: 0.9304259946720013\n",
      "---------------Classification_report------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7975    0.8813    0.8373      1019\n",
      "           1     0.8416    0.6953    0.7615       558\n",
      "           2     0.7954    0.6509    0.7160       424\n",
      "           3     0.8235    0.6306    0.7143       222\n",
      "           4     0.8611    0.9713    0.9129       383\n",
      "           5     0.7778    0.5185    0.6222        27\n",
      "           6     0.7742    0.8276    0.8000        58\n",
      "           7     0.8521    0.8090    0.8300       178\n",
      "           8     0.8351    0.9137    0.8727       510\n",
      "           9     0.8480    0.8030    0.8249       132\n",
      "          10     0.8817    0.8542    0.8677        96\n",
      "          11     0.6897    0.7595    0.7229       158\n",
      "          12     0.7954    0.9450    0.8637       218\n",
      "          13     1.0000    0.2667    0.4211        15\n",
      "\n",
      "    accuracy                         0.8164      3998\n",
      "   macro avg     0.8267    0.7519    0.7691      3998\n",
      "weighted avg     0.8178    0.8164    0.8121      3998\n",
      "\n",
      "RMSE: 0.13901689238660042\n",
      "Logloss: 0.5578185375917668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12794, 200) (12794, 14)\n",
      "(3199, 200) (3199, 14)\n",
      "(12794, 200) (12794, 14)\n",
      "(3199, 200) (3199, 14)\n",
      "(12794, 200) (12794, 14)\n",
      "(3199, 200) (3199, 14)\n",
      "(12795, 200) (12795, 14)\n",
      "(3198, 200) (3198, 14)\n",
      "(12795, 200) (12795, 14)\n",
      "(3198, 200) (3198, 14)\n",
      "Fold 4:\n",
      "---------------Classification_report------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7270    0.7719    0.7488      4037\n",
      "           1     0.6677    0.6535    0.6605      2202\n",
      "           2     0.6355    0.5394    0.5835      1739\n",
      "           3     0.6183    0.4781    0.5392       891\n",
      "           4     0.7820    0.9073    0.8400      1542\n",
      "           5     0.6296    0.3400    0.4416       100\n",
      "           6     0.5854    0.5195    0.5505       231\n",
      "           7     0.7243    0.6662    0.6941       710\n",
      "           8     0.7582    0.8170    0.7865      2088\n",
      "           9     0.8408    0.7529    0.7944       526\n",
      "          10     0.7616    0.6721    0.7141       366\n",
      "          11     0.5697    0.5891    0.5793       645\n",
      "          12     0.7306    0.8226    0.7739       857\n",
      "          13     0.6000    0.1017    0.1739        59\n",
      "\n",
      "    accuracy                         0.7118     15993\n",
      "   macro avg     0.6879    0.6165    0.6343     15993\n",
      "weighted avg     0.7074    0.7118    0.7066     15993\n",
      "\n",
      "RMSE: 0.17329694\n",
      "Logloss: 0.9355582435865847\n",
      "---------------Classification_report------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8022    0.8557    0.8281      1019\n",
      "           1     0.8123    0.7832    0.7974       558\n",
      "           2     0.7557    0.7075    0.7308       424\n",
      "           3     0.8212    0.5611    0.6667       221\n",
      "           4     0.8241    0.9635    0.8884       384\n",
      "           5     0.7059    0.4615    0.5581        26\n",
      "           6     0.9189    0.5763    0.7083        59\n",
      "           7     0.8011    0.7921    0.7966       178\n",
      "           8     0.8471    0.9020    0.8737       510\n",
      "           9     0.9016    0.8333    0.8661       132\n",
      "          10     0.9487    0.7708    0.8506        96\n",
      "          11     0.7208    0.7070    0.7138       157\n",
      "          12     0.7942    0.8813    0.8355       219\n",
      "          13     1.0000    0.4000    0.5714        15\n",
      "\n",
      "    accuracy                         0.8114      3998\n",
      "   macro avg     0.8324    0.7282    0.7633      3998\n",
      "weighted avg     0.8125    0.8114    0.8080      3998\n",
      "\n",
      "RMSE: 0.13985114831280165\n",
      "Logloss: 0.5659471323046631\n"
     ]
    }
   ],
   "source": [
    "for i, (holdout_train_index, holdout_index) in enumerate(stratify(df)):\n",
    "    oof_true, oof_pred, Y_holdout, holdout_predict = run_prediction(df, holdout_train_index, holdout_index)\n",
    "    print(f'Fold {i}:')\n",
    "    scoring(oof_true, oof_pred)\n",
    "    scoring(Y_holdout, holdout_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
